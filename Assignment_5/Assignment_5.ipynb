{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bYbNQzK6kj94"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from functools import partial\n",
    "from pathlib import Path \n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation, GlobalAveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "from keras.applications import DenseNet121\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.applications.densenet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 16 \n",
    "epochs = 5\n",
    "data_augmentation = True\n",
    "img_size = 200\n",
    "img_depth = 3\n",
    "# Model name, depth\n",
    "model_type = 'DenseNet121'\n",
    "\n",
    "MDL_OPS = [\"gender_output\", \"image_quality_output\", \"age_output\", \"weight_output\", \n",
    "           \"bag_output\", \"footwear_output\", \"pose_output\", \"emotion_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/akash/Documents/EIP4/Session5/hvc_data/hvc_annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>gender</th>\n",
       "      <th>imagequality</th>\n",
       "      <th>age</th>\n",
       "      <th>weight</th>\n",
       "      <th>carryingbag</th>\n",
       "      <th>footwear</th>\n",
       "      <th>emotion</th>\n",
       "      <th>bodypose</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>images/Set1/5580_2 (3).jpg</td>\n",
       "      <td>male</td>\n",
       "      <td>Average</td>\n",
       "      <td>35-45</td>\n",
       "      <td>normal-healthy</td>\n",
       "      <td>Grocery/Home/Plastic Bag</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Front-Frontish</td>\n",
       "      <td>resized/1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>images/Set1/4650_1 (4).jpg</td>\n",
       "      <td>female</td>\n",
       "      <td>Average</td>\n",
       "      <td>35-45</td>\n",
       "      <td>over-weight</td>\n",
       "      <td>None</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Angry/Serious</td>\n",
       "      <td>Front-Frontish</td>\n",
       "      <td>resized/2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>images/Set1/44880_0.jpg</td>\n",
       "      <td>male</td>\n",
       "      <td>Good</td>\n",
       "      <td>45-55</td>\n",
       "      <td>normal-healthy</td>\n",
       "      <td>Grocery/Home/Plastic Bag</td>\n",
       "      <td>CantSee</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Front-Frontish</td>\n",
       "      <td>resized/3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>images/Set1/26130_2.jpg</td>\n",
       "      <td>male</td>\n",
       "      <td>Good</td>\n",
       "      <td>45-55</td>\n",
       "      <td>normal-healthy</td>\n",
       "      <td>Daily/Office/Work Bag</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Front-Frontish</td>\n",
       "      <td>resized/4.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>images/Set1/IMG (4438).jpg</td>\n",
       "      <td>female</td>\n",
       "      <td>Good</td>\n",
       "      <td>35-45</td>\n",
       "      <td>slightly-overweight</td>\n",
       "      <td>None</td>\n",
       "      <td>CantSee</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Front-Frontish</td>\n",
       "      <td>resized/5.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     filename  gender imagequality    age  \\\n",
       "0  images/Set1/5580_2 (3).jpg    male      Average  35-45   \n",
       "1  images/Set1/4650_1 (4).jpg  female      Average  35-45   \n",
       "2     images/Set1/44880_0.jpg    male         Good  45-55   \n",
       "3     images/Set1/26130_2.jpg    male         Good  45-55   \n",
       "4  images/Set1/IMG (4438).jpg  female         Good  35-45   \n",
       "\n",
       "                weight               carryingbag footwear        emotion  \\\n",
       "0       normal-healthy  Grocery/Home/Plastic Bag   Normal        Neutral   \n",
       "1          over-weight                      None   Normal  Angry/Serious   \n",
       "2       normal-healthy  Grocery/Home/Plastic Bag  CantSee        Neutral   \n",
       "3       normal-healthy     Daily/Office/Work Bag   Normal        Neutral   \n",
       "4  slightly-overweight                      None  CantSee        Neutral   \n",
       "\n",
       "         bodypose     image_path  \n",
       "0  Front-Frontish  resized/1.jpg  \n",
       "1  Front-Frontish  resized/2.jpg  \n",
       "2  Front-Frontish  resized/3.jpg  \n",
       "3  Front-Frontish  resized/4.jpg  \n",
       "4  Front-Frontish  resized/5.jpg  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "38WJyDm3JnRh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <td>13573</td>\n",
       "      <td>13573</td>\n",
       "      <td>images/Set1/26700_3.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>13573</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>7636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imagequality</th>\n",
       "      <td>13573</td>\n",
       "      <td>3</td>\n",
       "      <td>Average</td>\n",
       "      <td>7509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>13573</td>\n",
       "      <td>5</td>\n",
       "      <td>25-35</td>\n",
       "      <td>5411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>13573</td>\n",
       "      <td>4</td>\n",
       "      <td>normal-healthy</td>\n",
       "      <td>8628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carryingbag</th>\n",
       "      <td>13573</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>7649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>footwear</th>\n",
       "      <td>13573</td>\n",
       "      <td>3</td>\n",
       "      <td>Normal</td>\n",
       "      <td>6038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emotion</th>\n",
       "      <td>13573</td>\n",
       "      <td>4</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>9660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bodypose</th>\n",
       "      <td>13573</td>\n",
       "      <td>3</td>\n",
       "      <td>Front-Frontish</td>\n",
       "      <td>8383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_path</th>\n",
       "      <td>13573</td>\n",
       "      <td>13573</td>\n",
       "      <td>resized/9137.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count unique                      top  freq\n",
       "filename      13573  13573  images/Set1/26700_3.jpg     1\n",
       "gender        13573      2                     male  7636\n",
       "imagequality  13573      3                  Average  7509\n",
       "age           13573      5                    25-35  5411\n",
       "weight        13573      4           normal-healthy  8628\n",
       "carryingbag   13573      3                     None  7649\n",
       "footwear      13573      3                   Normal  6038\n",
       "emotion       13573      4                  Neutral  9660\n",
       "bodypose      13573      3           Front-Frontish  8383\n",
       "image_path    13573  13573         resized/9137.jpg     1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "xEEqKK4bGAXP"
   },
   "outputs": [],
   "source": [
    "def encode_multi_categories(x, n=3, order_dict=None):\n",
    "    \"\"\"`x` is numpy array\"\"\"\n",
    "    if order_dict is None:\n",
    "        x_encoded = LabelEncoder().fit_transform(x)\n",
    "        return np.eye(n)[x_encoded]\n",
    "    return np.eye(n)[order_dict[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "X6uLCwCnxr4X"
   },
   "outputs": [],
   "source": [
    "def resize_and_pad(image, size=224, fill=0):\n",
    "    orig_size = image.shape[:2] \n",
    "    ratio = float(size)/max(orig_size)\n",
    "    new_size = tuple([int(x*ratio) for x in orig_size])\n",
    "\n",
    "    image = cv2.resize(image, (new_size[1], new_size[0]))\n",
    "\n",
    "    delta_w = size - new_size[1]\n",
    "    delta_h = size - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "\n",
    "    color = [fill]*3\n",
    "    return cv2.copyMakeBorder(\n",
    "        image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "YAeJPaWz4R1u"
   },
   "outputs": [],
   "source": [
    "def read_image(filename, resize=None, augment_fn=None):\n",
    "    image = cv2.imread(filename)\n",
    "    b,g,r = cv2.split(image)       # get b,g,r\n",
    "    image = cv2.merge([r,g,b])     # switch it to rgb\n",
    "    if resize is not None:\n",
    "        image = resize_and_pad(image, resize)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/akash/Documents/EIP4/Session5/hvc_data'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/Users/akash/Documents/EIP4/Session5/hvc_data/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_kfa6QB4hsE"
   },
   "outputs": [],
   "source": [
    "x = read_image(df.image_path[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IpUqDqcT46F9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filename        images/Set1/32490_1.jpg\n",
       "gender                             male\n",
       "imagequality                       Good\n",
       "age                               35-45\n",
       "weight              slightly-overweight\n",
       "carryingbag                        None\n",
       "footwear                         Normal\n",
       "emotion                         Neutral\n",
       "bodypose                 Front-Frontish\n",
       "image_path                resized/6.jpg\n",
       "Name: 5, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBYEOHW5zdEGgQ4fbHF0Q6NDhNsf/B4qR0m6ADyXLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "TRmb0KEK6V58"
   },
   "outputs": [],
   "source": [
    "X = df.image_path.apply(partial(read_image, resize=img_size)).values\n",
    "X = np.stack(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13573, 200, 200, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "g7FHbX6t7GjP"
   },
   "outputs": [],
   "source": [
    "# gender\n",
    "gender_dict = {\"female\": 0, \"male\": 1}\n",
    "y_gender = np.stack(\n",
    "    df.gender.apply(partial(encode_multi_categories, n=2, order_dict=gender_dict)).values\n",
    ")\n",
    "\n",
    "# image quality\n",
    "image_encode_dict = dict(zip((\"Bad\", \"Average\", \"Good\"), range(3)))\n",
    "y_image_quality = np.stack(\n",
    "    df.imagequality.apply(partial(encode_multi_categories, n=3, order_dict=image_encode_dict)).values\n",
    ")\n",
    "\n",
    "# age\n",
    "unique_ages = df.age.unique()\n",
    "age_dict = dict(zip(sorted(unique_ages),range(len(unique_ages))))\n",
    "y_age = np.stack(\n",
    "    df.age.apply(partial(encode_multi_categories, n=len(unique_ages), order_dict=age_dict)).values\n",
    ")\n",
    "\n",
    "# weightweight_dict\n",
    "unique_weight = ['underweight','normal-healthy', 'slightly-overweight', 'over-weight']\n",
    "weight_dict = dict(zip(unique_weight, range(len(unique_weight))))\n",
    "y_weight = np.stack(\n",
    "    df.weight.apply(partial(encode_multi_categories, n=len(unique_weight), order_dict=weight_dict)).values\n",
    ")\n",
    "\n",
    "\n",
    "# bag\n",
    "unique_bags = df.carryingbag.unique()\n",
    "bag_dict = dict(zip(sorted(unique_bags),range(len(unique_bags))))\n",
    "y_bag = np.stack(\n",
    "    df.carryingbag.apply(partial(encode_multi_categories, n=len(unique_bags), order_dict=bag_dict)).values\n",
    ")\n",
    "\n",
    "# pose\n",
    "unique_poses = df.bodypose.unique()\n",
    "pose_dict = dict(zip(sorted(unique_poses),range(len(unique_poses))))\n",
    "y_pose = np.stack(\n",
    "    df.bodypose.apply(partial(encode_multi_categories, n=len(unique_poses), order_dict=pose_dict)).values\n",
    ")\n",
    "\n",
    "# footwear\n",
    "unique_footwears = df.footwear.unique()\n",
    "footwear_dict = dict(zip(sorted(unique_footwears),range(len(unique_footwears))))\n",
    "y_footwear = np.stack(\n",
    "    df.footwear.apply(partial(encode_multi_categories, n=len(unique_footwears), order_dict=footwear_dict)).values\n",
    ")\n",
    "\n",
    "# emotion\n",
    "unique_emotions = df.emotion.unique()\n",
    "emotion_dict = dict(zip(unique_emotions, range(len(unique_emotions))))\n",
    "y_emotion = np.stack(\n",
    "    df.emotion.apply(partial(encode_multi_categories, n=len(unique_emotions), order_dict=emotion_dict)).values\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "AWU_9vSO7I5Y"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train_idx, y_valid_idx = train_test_split(X, range(len(X)), random_state=67)\n",
    "\n",
    "y_train = {\n",
    "    \"gender_output\": y_gender[y_train_idx], \n",
    "    \"image_quality_output\": y_image_quality[y_train_idx],\n",
    "    \"age_output\": y_age[y_train_idx],\n",
    "    \"weight_output\": y_weight[y_train_idx],\n",
    "    \"bag_output\": y_bag[y_train_idx],\n",
    "    \"pose_output\": y_pose[y_train_idx],\n",
    "    \"footwear_output\": y_footwear[y_train_idx],\n",
    "    \"emotion_output\": y_emotion[y_train_idx],\n",
    "\n",
    "}\n",
    "\n",
    "y_test = {\n",
    "    \"gender_output\": y_gender[y_valid_idx], \n",
    "    \"image_quality_output\": y_image_quality[y_valid_idx],\n",
    "    \"age_output\": y_age[y_valid_idx],\n",
    "    \"weight_output\": y_weight[y_valid_idx],\n",
    "    \"bag_output\": y_bag[y_valid_idx],\n",
    "    \"pose_output\": y_pose[y_valid_idx],\n",
    "    \"footwear_output\": y_footwear[y_valid_idx],\n",
    "    \"emotion_output\": y_emotion[y_valid_idx],\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_class_weights(y_df, output_type):\n",
    "    percentage = pd.DataFrame(np.argmax(y_df[output_type], axis=1))[0].value_counts(normalize=True)\n",
    "    return dict(percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.43707633362805776, 1: 0.5629236663719422}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_type = \"gender_output\"\n",
    "gender_class_weight = get_class_weights(y_train, output_type)\n",
    "gender_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.16406326751154338, 1: 0.5513311720208272, 2: 0.2846055604676294}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_type = \"image_quality_output\"\n",
    "image_quality_class_weight = get_class_weights(y_train, output_type)\n",
    "image_quality_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.18331859711170057,\n",
       " 1: 0.4026918164849199,\n",
       " 2: 0.25228411435307985,\n",
       " 3: 0.10777090087434915,\n",
       " 4: 0.053934571175950484}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_type = \"age_output\"\n",
    "age_class_weight = get_class_weights(y_train, output_type)\n",
    "age_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.06444640927399548,\n",
       " 1: 0.6361135671480499,\n",
       " 2: 0.23381471657333727,\n",
       " 3: 0.06562530700461736}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_type = \"weight_output\"\n",
    "weight_class_weight = get_class_weights(y_train, output_type)\n",
    "weight_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.3410944100599273, 1: 0.09608016504568229, 2: 0.5628254248943904}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_type = \"bag_output\"\n",
    "bag_class_weight = get_class_weights(y_train, output_type)\n",
    "bag_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.16386678455643972, 1: 0.6160723057274782, 2: 0.22006090971608214}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_type = \"pose_output\"\n",
    "pose_class_weight = get_class_weights(y_train, output_type)\n",
    "pose_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.3708615777581295, 1: 0.18597111700559976, 2: 0.44316730523627074}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_type = \"footwear_output\"\n",
    "footwear_class_weight = get_class_weights(y_train, output_type)\n",
    "footwear_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.7117595048629531,\n",
       " 1: 0.1111111111111111,\n",
       " 2: 0.11975636113567148,\n",
       " 3: 0.05737302289026427}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_type = \"emotion_output\"\n",
    "emotion_class_weight = get_class_weights(y_train, output_type)\n",
    "emotion_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_weights = {\"gender_output\": gender_class_weight,\n",
    "                 \"image_quality_output\": image_quality_class_weight,\n",
    "                 \"age_output\": age_class_weight,\n",
    "                 \"weight_output\": weight_class_weight,\n",
    "                 \"bag_output\": bag_class_weight,\n",
    "                 \"pose_output\": pose_class_weight,\n",
    "                 \"footwear_output\": footwear_class_weight,\n",
    "                 \"emotion_output\": emotion_class_weight,\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age_output': {0: 0.18331859711170057,\n",
       "  1: 0.4026918164849199,\n",
       "  2: 0.25228411435307985,\n",
       "  3: 0.10777090087434915,\n",
       "  4: 0.053934571175950484},\n",
       " 'bag_output': {0: 0.3410944100599273,\n",
       "  1: 0.09608016504568229,\n",
       "  2: 0.5628254248943904},\n",
       " 'emotion_output': {0: 0.7117595048629531,\n",
       "  1: 0.1111111111111111,\n",
       "  2: 0.11975636113567148,\n",
       "  3: 0.05737302289026427},\n",
       " 'footwear_output': {0: 0.3708615777581295,\n",
       "  1: 0.18597111700559976,\n",
       "  2: 0.44316730523627074},\n",
       " 'gender_output': {0: 0.43707633362805776, 1: 0.5629236663719422},\n",
       " 'image_quality_output': {0: 0.16406326751154338,\n",
       "  1: 0.5513311720208272,\n",
       "  2: 0.2846055604676294},\n",
       " 'pose_output': {0: 0.16386678455643972,\n",
       "  1: 0.6160723057274782,\n",
       "  2: 0.22006090971608214},\n",
       " 'weight_output': {0: 0.06444640927399548,\n",
       "  1: 0.6361135671480499,\n",
       "  2: 0.23381471657333727,\n",
       "  3: 0.06562530700461736}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "#Have taken this class from https://github.com/titu1994/keras-one-cycle\n",
    "class OneCycleLR(Callback):\n",
    "    def __init__(self,\n",
    "                 epochs,\n",
    "                 batch_size,\n",
    "                 samples,\n",
    "                 steps,\n",
    "                 max_lr,\n",
    "                 end_percentage=0.1,\n",
    "                 scale_percentage=None,\n",
    "                 maximum_momentum=0.95,\n",
    "                 minimum_momentum=0.85,\n",
    "                 verbose=True):\n",
    "        \"\"\" This callback implements a cyclical learning rate policy (CLR).\n",
    "        This is a special case of Cyclic Learning Rates, where we have only 1 cycle.\n",
    "        After the completion of 1 cycle, the learning rate will decrease rapidly to\n",
    "        100th its initial lowest value.\n",
    "        # Arguments:\n",
    "            max_lr: Float. Initial learning rate. This also sets the\n",
    "                starting learning rate (which will be 10x smaller than\n",
    "                this), and will increase to this value during the first cycle.\n",
    "            end_percentage: Float. The percentage of all the epochs of training\n",
    "                that will be dedicated to sharply decreasing the learning\n",
    "                rate after the completion of 1 cycle. Must be between 0 and 1.\n",
    "            scale_percentage: Float or None. If float, must be between 0 and 1.\n",
    "                If None, it will compute the scale_percentage automatically\n",
    "                based on the `end_percentage`.\n",
    "            maximum_momentum: Optional. Sets the maximum momentum (initial)\n",
    "                value, which gradually drops to its lowest value in half-cycle,\n",
    "                then gradually increases again to stay constant at this max value.\n",
    "                Can only be used with SGD Optimizer.\n",
    "            minimum_momentum: Optional. Sets the minimum momentum at the end of\n",
    "                the half-cycle. Can only be used with SGD Optimizer.\n",
    "            verbose: Bool. Whether to print the current learning rate after every\n",
    "                epoch.\n",
    "        # Reference\n",
    "            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n",
    "            - [Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120)\n",
    "        \"\"\"\n",
    "        super(OneCycleLR, self).__init__()\n",
    "\n",
    "        if end_percentage < 0. or end_percentage > 1.:\n",
    "            raise ValueError(\"`end_percentage` must be between 0 and 1\")\n",
    "\n",
    "        if scale_percentage is not None and (scale_percentage < 0. or scale_percentage > 1.):\n",
    "            raise ValueError(\"`scale_percentage` must be between 0 and 1\")\n",
    "\n",
    "        self.initial_lr = max_lr\n",
    "        self.end_percentage = end_percentage\n",
    "        self.scale = float(scale_percentage) if scale_percentage is not None else float(end_percentage)\n",
    "        self.max_momentum = maximum_momentum\n",
    "        self.min_momentum = minimum_momentum\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if self.max_momentum is not None and self.min_momentum is not None:\n",
    "            self._update_momentum = True\n",
    "        else:\n",
    "            self._update_momentum = False\n",
    "\n",
    "        self.clr_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.samples = samples\n",
    "        self.steps = steps\n",
    "        self.num_iterations = None\n",
    "        self.mid_cycle_id = None\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"\n",
    "        Reset the callback.\n",
    "        \"\"\"\n",
    "        self.clr_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "    def compute_lr(self):\n",
    "        \"\"\"\n",
    "        Compute the learning rate based on which phase of the cycle it is in.\n",
    "        - If in the first half of training, the learning rate gradually increases.\n",
    "        - If in the second half of training, the learning rate gradually decreases.\n",
    "        - If in the final `end_percentage` portion of training, the learning rate\n",
    "            is quickly reduced to near 100th of the original min learning rate.\n",
    "        # Returns:\n",
    "            the new learning rate\n",
    "        \"\"\"\n",
    "        if self.clr_iterations > 2 * self.mid_cycle_id:\n",
    "            current_percentage = (self.clr_iterations - 2 * self.mid_cycle_id)\n",
    "            current_percentage /= float((self.num_iterations - 2 * self.mid_cycle_id))\n",
    "            new_lr = self.initial_lr * (1. + (current_percentage *\n",
    "                                              (1. - 100.) / 100.)) * self.scale\n",
    "\n",
    "        elif self.clr_iterations > self.mid_cycle_id:\n",
    "            current_percentage = 1. - (\n",
    "                self.clr_iterations - self.mid_cycle_id) / self.mid_cycle_id\n",
    "            new_lr = self.initial_lr * (1. + current_percentage *\n",
    "                                        (self.scale * 100 - 1.)) * self.scale\n",
    "\n",
    "        else:\n",
    "            current_percentage = self.clr_iterations / self.mid_cycle_id\n",
    "            new_lr = self.initial_lr * (1. + current_percentage *\n",
    "                                        (self.scale * 100 - 1.)) * self.scale\n",
    "\n",
    "        if self.clr_iterations == self.num_iterations:\n",
    "            self.clr_iterations = 0\n",
    "\n",
    "        return new_lr\n",
    "\n",
    "    def compute_momentum(self):\n",
    "        \"\"\"\n",
    "         Compute the momentum based on which phase of the cycle it is in.\n",
    "        - If in the first half of training, the momentum gradually decreases.\n",
    "        - If in the second half of training, the momentum gradually increases.\n",
    "        - If in the final `end_percentage` portion of training, the momentum value\n",
    "            is kept constant at the maximum initial value.\n",
    "        # Returns:\n",
    "            the new momentum value\n",
    "        \"\"\"\n",
    "        if self.clr_iterations > 2 * self.mid_cycle_id:\n",
    "            new_momentum = self.max_momentum\n",
    "\n",
    "        elif self.clr_iterations > self.mid_cycle_id:\n",
    "            current_percentage = 1. - ((self.clr_iterations - self.mid_cycle_id) / float(\n",
    "                                        self.mid_cycle_id))\n",
    "            new_momentum = self.max_momentum - current_percentage * (\n",
    "                self.max_momentum - self.min_momentum)\n",
    "\n",
    "        else:\n",
    "            current_percentage = self.clr_iterations / float(self.mid_cycle_id)\n",
    "            new_momentum = self.max_momentum - current_percentage * (\n",
    "                self.max_momentum - self.min_momentum)\n",
    "\n",
    "        return new_momentum\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        \n",
    "        if self.steps is not None:\n",
    "            self.num_iterations = self.epochs * self.steps\n",
    "        else:\n",
    "            if (self.samples % self.batch_size) == 0:\n",
    "                remainder = 0\n",
    "            else:\n",
    "                remainder = 1\n",
    "            self.num_iterations = (self.epochs + remainder) * self.samples // self.batch_size\n",
    "\n",
    "        self.mid_cycle_id = int(self.num_iterations * ((1. - self.end_percentage)) / float(2))\n",
    "\n",
    "        self._reset()\n",
    "        K.set_value(self.model.optimizer.lr, self.compute_lr())\n",
    "\n",
    "        if self._update_momentum:\n",
    "            if not hasattr(self.model.optimizer, 'momentum'):\n",
    "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
    "\n",
    "            new_momentum = self.compute_momentum()\n",
    "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
    "\n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "\n",
    "        self.clr_iterations += 1\n",
    "        new_lr = self.compute_lr()\n",
    "\n",
    "        self.history.setdefault('lr', []).append(\n",
    "            K.get_value(self.model.optimizer.lr))\n",
    "        K.set_value(self.model.optimizer.lr, new_lr)\n",
    "\n",
    "        if self._update_momentum:\n",
    "            if not hasattr(self.model.optimizer, 'momentum'):\n",
    "                raise ValueError(\"Momentum can be updated only on SGD optimizer !\")\n",
    "\n",
    "            new_momentum = self.compute_momentum()\n",
    "\n",
    "            self.history.setdefault('momentum', []).append(\n",
    "                K.get_value(self.model.optimizer.momentum))\n",
    "            K.set_value(self.model.optimizer.momentum, new_momentum)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.verbose:\n",
    "            if self._update_momentum:\n",
    "                print(\" - lr: %0.5f - momentum: %0.2f \" %\n",
    "                      (self.history['lr'][-1], self.history['momentum'][-1]))\n",
    "\n",
    "            else:\n",
    "                print(\" - lr: %0.5f \" % (self.history['lr'][-1]))\n",
    "\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    def __init__(self,\n",
    "                 num_samples,\n",
    "                 batch_size,\n",
    "                 minimum_lr=1e-5,\n",
    "                 maximum_lr=10.,\n",
    "                 lr_scale='exp',\n",
    "                 validation_data=None,\n",
    "                 validation_sample_rate=5,\n",
    "                 stopping_criterion_factor=4.,\n",
    "                 loss_smoothing_beta=0.98,\n",
    "                 save_dir=None,\n",
    "                 verbose=True):\n",
    "        \"\"\"\n",
    "        This class uses the Cyclic Learning Rate history to find a\n",
    "        set of learning rates that can be good initializations for the\n",
    "        One-Cycle training proposed by Leslie Smith in the paper referenced\n",
    "        below.\n",
    "        A port of the Fast.ai implementation for Keras.\n",
    "        # Note\n",
    "        This requires that the model be trained for exactly 1 epoch. If the model\n",
    "        is trained for more epochs, then the metric calculations are only done for\n",
    "        the first epoch.\n",
    "        # Interpretation\n",
    "        Upon visualizing the loss plot, check where the loss starts to increase\n",
    "        rapidly. Choose a learning rate at somewhat prior to the corresponding\n",
    "        position in the plot for faster convergence. This will be the maximum_lr lr.\n",
    "        Choose the max value as this value when passing the `max_val` argument\n",
    "        to OneCycleLR callback.\n",
    "        Since the plot is in log-scale, you need to compute 10 ^ (-k) of the x-axis\n",
    "        # Arguments:\n",
    "            num_samples: Integer. Number of samples in the dataset.\n",
    "            batch_size: Integer. Batch size during training.\n",
    "            minimum_lr: Float. Initial learning rate (and the minimum).\n",
    "            maximum_lr: Float. Final learning rate (and the maximum).\n",
    "            lr_scale: Can be one of ['exp', 'linear']. Chooses the type of\n",
    "                scaling for each update to the learning rate during subsequent\n",
    "                batches. Choose 'exp' for large range and 'linear' for small range.\n",
    "            validation_data: Requires the validation dataset as a tuple of\n",
    "                (X, y) belonging to the validation set. If provided, will use the\n",
    "                validation set to compute the loss metrics. Else uses the training\n",
    "                batch loss. Will warn if not provided to alert the user.\n",
    "            validation_sample_rate: Positive or Negative Integer. Number of batches to sample from the\n",
    "                validation set per iteration of the LRFinder. Larger number of\n",
    "                samples will reduce the variance but will take longer time to execute\n",
    "                per batch.\n",
    "                If Positive > 0, will sample from the validation dataset\n",
    "                If Megative, will use the entire dataset\n",
    "            stopping_criterion_factor: Integer or None. A factor which is used\n",
    "                to measure large increase in the loss value during training.\n",
    "                Since callbacks cannot stop training of a model, it will simply\n",
    "                stop logging the additional values from the epochs after this\n",
    "                stopping criterion has been met.\n",
    "                If None, this check will not be performed.\n",
    "            loss_smoothing_beta: Float. The smoothing factor for the moving\n",
    "                average of the loss function.\n",
    "            save_dir: Optional, String. If passed a directory path, the callback\n",
    "                will save the running loss and learning rates to two separate numpy\n",
    "                arrays inside this directory. If the directory in this path does not\n",
    "                exist, they will be created.\n",
    "            verbose: Whether to print the learning rate after every batch of training.\n",
    "        # References:\n",
    "            - [A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, weight_decay, and weight decay](https://arxiv.org/abs/1803.09820)\n",
    "        \"\"\"\n",
    "        super(LRFinder, self).__init__()\n",
    "\n",
    "        if lr_scale not in ['exp', 'linear']:\n",
    "            raise ValueError(\"`lr_scale` must be one of ['exp', 'linear']\")\n",
    "\n",
    "        if validation_data is not None:\n",
    "            self.validation_data = validation_data\n",
    "            self.use_validation_set = True\n",
    "\n",
    "            if validation_sample_rate > 0 or validation_sample_rate < 0:\n",
    "                self.validation_sample_rate = validation_sample_rate\n",
    "            else:\n",
    "                raise ValueError(\"`validation_sample_rate` must be a positive or negative integer other than o\")\n",
    "        else:\n",
    "            self.use_validation_set = False\n",
    "            self.validation_sample_rate = 0\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "        self.batch_size = batch_size\n",
    "        self.initial_lr = minimum_lr\n",
    "        self.final_lr = maximum_lr\n",
    "        self.lr_scale = lr_scale\n",
    "        self.stopping_criterion_factor = stopping_criterion_factor\n",
    "        self.loss_smoothing_beta = loss_smoothing_beta\n",
    "        self.save_dir = save_dir\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.num_batches_ = num_samples // batch_size\n",
    "        self.current_lr_ = minimum_lr\n",
    "\n",
    "        if lr_scale == 'exp':\n",
    "            self.lr_multiplier_ = (maximum_lr / float(minimum_lr)) ** (\n",
    "                1. / float(self.num_batches_))\n",
    "        else:\n",
    "            extra_batch = int((num_samples % batch_size) != 0)\n",
    "            self.lr_multiplier_ = np.linspace(\n",
    "                minimum_lr, maximum_lr, num=self.num_batches_ + extra_batch)\n",
    "\n",
    "        # If negative, use entire validation set\n",
    "        if self.validation_sample_rate < 0:\n",
    "            self.validation_sample_rate = self.validation_data[0].shape[0] // batch_size\n",
    "\n",
    "        self.current_batch_ = 0\n",
    "        self.current_epoch_ = 0\n",
    "        self.best_loss_ = 1e6\n",
    "        self.running_loss_ = 0.\n",
    "\n",
    "        self.history = {}\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "\n",
    "        self.current_epoch_ = 1\n",
    "        K.set_value(self.model.optimizer.lr, self.initial_lr)\n",
    "\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.current_batch_ = 0\n",
    "\n",
    "        if self.current_epoch_ > 1:\n",
    "            warnings.warn(\n",
    "                \"\\n\\nLearning rate finder should be used only with a single epoch. \"\n",
    "                \"Hereafter, the callback will not measure the losses.\\n\\n\")\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        self.current_batch_ += 1\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if self.current_epoch_ > 1:\n",
    "            return\n",
    "\n",
    "        if self.use_validation_set:\n",
    "            X, Y = self.validation_data[0], self.validation_data[1]\n",
    "\n",
    "            # use 5 random batches from test set for fast approximate of loss\n",
    "            num_samples = self.batch_size * self.validation_sample_rate\n",
    "\n",
    "            if num_samples > X.shape[0]:\n",
    "                num_samples = X.shape[0]\n",
    "\n",
    "            idx = np.random.choice(X.shape[0], num_samples, replace=False)\n",
    "            x = X[idx]\n",
    "            y = Y[idx]\n",
    "\n",
    "            values = self.model.evaluate(x, y, batch_size=self.batch_size, verbose=False)\n",
    "            loss = values[0]\n",
    "        else:\n",
    "            loss = logs['loss']\n",
    "\n",
    "        # smooth the loss value and bias correct\n",
    "        running_loss = self.loss_smoothing_beta * loss + (\n",
    "            1. - self.loss_smoothing_beta) * loss\n",
    "        running_loss = running_loss / (\n",
    "            1. - self.loss_smoothing_beta**self.current_batch_)\n",
    "\n",
    "        # stop logging if loss is too large\n",
    "        if self.current_batch_ > 1 and self.stopping_criterion_factor is not None and (\n",
    "                running_loss >\n",
    "                self.stopping_criterion_factor * self.best_loss_):\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\" - LRFinder: Skipping iteration since loss is %d times as large as best loss (%0.4f)\"\n",
    "                      % (self.stopping_criterion_factor, self.best_loss_))\n",
    "            return\n",
    "\n",
    "        if running_loss < self.best_loss_ or self.current_batch_ == 1:\n",
    "            self.best_loss_ = running_loss\n",
    "\n",
    "        current_lr = K.get_value(self.model.optimizer.lr)\n",
    "\n",
    "        self.history.setdefault('running_loss_', []).append(running_loss)\n",
    "        if self.lr_scale == 'exp':\n",
    "            self.history.setdefault('log_lrs', []).append(np.log10(current_lr))\n",
    "        else:\n",
    "            self.history.setdefault('log_lrs', []).append(current_lr)\n",
    "\n",
    "        # compute the lr for the next batch and update the optimizer lr\n",
    "        if self.lr_scale == 'exp':\n",
    "            current_lr *= self.lr_multiplier_\n",
    "        else:\n",
    "            current_lr = self.lr_multiplier_[self.current_batch_ - 1]\n",
    "\n",
    "        K.set_value(self.model.optimizer.lr, current_lr)\n",
    "\n",
    "        # save the other metrics as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        if self.verbose:\n",
    "            if self.use_validation_set:\n",
    "                print(\" - LRFinder: val_loss: %1.4f - lr = %1.8f \" %\n",
    "                      (values[0], current_lr))\n",
    "            else:\n",
    "                print(\" - LRFinder: lr = %1.8f \" % current_lr)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.save_dir is not None and self.current_epoch_ <= 1:\n",
    "            if not os.path.exists(self.save_dir):\n",
    "                os.makedirs(self.save_dir)\n",
    "\n",
    "            losses_path = os.path.join(self.save_dir, 'losses.npy')\n",
    "            lrs_path = os.path.join(self.save_dir, 'lrs.npy')\n",
    "\n",
    "            np.save(losses_path, self.losses)\n",
    "            np.save(lrs_path, self.lrs)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\"\\tLR Finder : Saved the losses and learning rate values in path : {%s}\"\n",
    "                      % (self.save_dir))\n",
    "\n",
    "        self.current_epoch_ += 1\n",
    "\n",
    "        warnings.simplefilter(\"default\")\n",
    "\n",
    "    def plot_schedule(self, clip_beginning=None, clip_endding=None):\n",
    "        \"\"\"\n",
    "        Plots the schedule from the callback itself.\n",
    "        # Arguments:\n",
    "            clip_beginning: Integer or None. If positive integer, it will\n",
    "                remove the specified portion of the loss graph to remove the large\n",
    "                loss values in the beginning of the graph.\n",
    "            clip_endding: Integer or None. If negative integer, it will\n",
    "                remove the specified portion of the ending of the loss graph to\n",
    "                remove the sharp increase in the loss values at high learning rates.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.style.use('seaborn-white')\n",
    "        except ImportError:\n",
    "            print(\n",
    "                \"Matplotlib not found. Please use `pip install matplotlib` first.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        if clip_beginning is not None and clip_beginning < 0:\n",
    "            clip_beginning = -clip_beginning\n",
    "\n",
    "        if clip_endding is not None and clip_endding > 0:\n",
    "            clip_endding = -clip_endding\n",
    "\n",
    "        losses = self.losses\n",
    "        lrs = self.lrs\n",
    "\n",
    "        if clip_beginning:\n",
    "            losses = losses[clip_beginning:]\n",
    "            lrs = lrs[clip_beginning:]\n",
    "\n",
    "        if clip_endding:\n",
    "            losses = losses[:clip_endding]\n",
    "            lrs = lrs[:clip_endding]\n",
    "\n",
    "        plt.plot(lrs, losses)\n",
    "        plt.title('Learning rate vs Loss')\n",
    "        plt.xlabel('learning rate')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "\n",
    "    @classmethod\n",
    "    def restore_schedule_from_dir(cls,\n",
    "                                  directory,\n",
    "                                  clip_beginning=None,\n",
    "                                  clip_endding=None):\n",
    "        \"\"\"\n",
    "        Loads the training history from the saved numpy files in the given directory.\n",
    "        # Arguments:\n",
    "            directory: String. Path to the directory where the serialized numpy\n",
    "                arrays of the loss and learning rates are saved.\n",
    "            clip_beginning: Integer or None. If positive integer, it will\n",
    "                remove the specified portion of the loss graph to remove the large\n",
    "                loss values in the beginning of the graph.\n",
    "            clip_endding: Integer or None. If negative integer, it will\n",
    "                remove the specified portion of the ending of the loss graph to\n",
    "                remove the sharp increase in the loss values at high learning rates.\n",
    "        Returns:\n",
    "            tuple of (losses, learning rates)\n",
    "        \"\"\"\n",
    "        if clip_beginning is not None and clip_beginning < 0:\n",
    "            clip_beginning = -clip_beginning\n",
    "\n",
    "        if clip_endding is not None and clip_endding > 0:\n",
    "            clip_endding = -clip_endding\n",
    "\n",
    "        losses_path = os.path.join(directory, 'losses.npy')\n",
    "        lrs_path = os.path.join(directory, 'lrs.npy')\n",
    "\n",
    "        if not os.path.exists(losses_path) or not os.path.exists(lrs_path):\n",
    "            print(\"%s and %s could not be found at directory : {%s}\" %\n",
    "                  (losses_path, lrs_path, directory))\n",
    "\n",
    "            losses = None\n",
    "            lrs = None\n",
    "\n",
    "        else:\n",
    "            losses = np.load(losses_path)\n",
    "            lrs = np.load(lrs_path)\n",
    "\n",
    "            if clip_beginning:\n",
    "                losses = losses[clip_beginning:]\n",
    "                lrs = lrs[clip_beginning:]\n",
    "\n",
    "            if clip_endding:\n",
    "                losses = losses[:clip_endding]\n",
    "                lrs = lrs[:clip_endding]\n",
    "\n",
    "        return losses, lrs\n",
    "\n",
    "    @classmethod\n",
    "    def plot_schedule_from_file(cls,\n",
    "                                directory,\n",
    "                                clip_beginning=None,\n",
    "                                clip_endding=None):\n",
    "        \"\"\"\n",
    "        Plots the schedule from the saved numpy arrays of the loss and learning\n",
    "        rate values in the specified directory.\n",
    "        # Arguments:\n",
    "            directory: String. Path to the directory where the serialized numpy\n",
    "                arrays of the loss and learning rates are saved.\n",
    "            clip_beginning: Integer or None. If positive integer, it will\n",
    "                remove the specified portion of the loss graph to remove the large\n",
    "                loss values in the beginning of the graph.\n",
    "            clip_endding: Integer or None. If negative integer, it will\n",
    "                remove the specified portion of the ending of the loss graph to\n",
    "                remove the sharp increase in the loss values at high learning rates.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.style.use('seaborn-white')\n",
    "        except ImportError:\n",
    "            print(\"Matplotlib not found. Please use `pip install matplotlib` first.\")\n",
    "            return\n",
    "\n",
    "        losses, lrs = cls.restore_schedule_from_dir(\n",
    "            directory,\n",
    "            clip_beginning=clip_beginning,\n",
    "            clip_endding=clip_endding)\n",
    "\n",
    "        if losses is None or lrs is None:\n",
    "            return\n",
    "        else:\n",
    "            plt.plot(lrs, losses)\n",
    "            plt.title('Learning rate vs Loss')\n",
    "            plt.xlabel('learning rate')\n",
    "            plt.ylabel('loss')\n",
    "            plt.show()\n",
    "\n",
    "    @property\n",
    "    def lrs(self):\n",
    "        return np.array(self.history['log_lrs'])\n",
    "\n",
    "    @property\n",
    "    def losses(self):\n",
    "        return np.array(self.history['running_loss_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_eraser(p=0.8, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(input_img):\n",
    "        img_h, img_w, img_c = input_img.shape\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w, :] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eraser = get_random_eraser(pixel_level=True)\n",
    "def aug_fn(x):\n",
    "    x = eraser(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(x, y, batchsize=32, train_mode=True):\n",
    "    datagen = ImageDataGenerator(\n",
    "        # set input mean to 0 over the dataset\n",
    "        featurewise_center=False,\n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=False,\n",
    "        # divide inputs by std of dataset\n",
    "        featurewise_std_normalization=False,\n",
    "        # divide each input by its std\n",
    "        samplewise_std_normalization=False,\n",
    "        # apply ZCA whitening\n",
    "        zca_whitening=False,\n",
    "        # epsilon for ZCA whitening\n",
    "        #zca_epsilon=1e-06,\n",
    "        # randomly rotate images in the range (deg 0 to 180)\n",
    "        rotation_range=0.01,\n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.01,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.01,\n",
    "        # set range for random shear\n",
    "        shear_range=0.,\n",
    "        # set range for random zoom\n",
    "        zoom_range=0.,\n",
    "        # set range for random channel shifts\n",
    "        channel_shift_range=0.,\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        # value used for fill_mode = \"constant\"\n",
    "        cval=0.,\n",
    "        # randomly flip images\n",
    "        horizontal_flip=False,\n",
    "        # randomly flip images\n",
    "        vertical_flip=True,\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        #preprocessing_function=get_random_eraser(v_l=0, v_h=24),\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0\n",
    "    )                                                                   \n",
    "    while True:\n",
    "        # Create a batch-indexer to populate the image & correspnding labels\n",
    "        # batch-wise\n",
    "        batch_nums = np.random.randint(0, x.shape[0], size=batchsize)\n",
    "        # Create placeholders for the returned (batch_images & part-of-batch_labels)\n",
    "        # & single label output.\n",
    "        batch_images = np.zeros((batchsize, img_size, img_size, img_depth))\n",
    "        batch_labels = {j: [] for j in MDL_OPS }\n",
    "        for i in range(batchsize):\n",
    "            # get an image \n",
    "            image = x[batch_nums[i]]\n",
    "            if train_mode:\n",
    "                image = aug_fn(image)\n",
    "                #image = datagen.random_transform(image)\n",
    "            batch_images[i] = image\n",
    "            for j in MDL_OPS:\n",
    "                batch_labels[j].append(y[j][batch_nums[i]])\n",
    "        for j in MDL_OPS:\n",
    "            batch_labels[j] = np.array(batch_labels[j])\n",
    "        batch_images = preprocess_input(batch_images)\n",
    "        yield batch_images, [batch_labels[j] for j in MDL_OPS] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iterator = batch_generator(x_train, y_train, batchsize=batch_size)\n",
    "test_iterator = batch_generator(x_test, y_test, batchsize=batch_size, train_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Activation\n",
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "03W8Pagg_Ppp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "29089792/29084464 [==============================] - 56s 2us/step\n"
     ]
    }
   ],
   "source": [
    "backbone = DenseNet121(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=input_shape))\n",
    "neck = backbone.output\n",
    "\n",
    "def build_tower(in_layer, num_classes, activation, name):\n",
    "    neck = Conv2D(num_classes, 1, use_bias=False)(in_layer)\n",
    "    neck = GlobalAveragePooling2D()(neck)\n",
    "    neck = Activation(activation, name=name)(neck)\n",
    "    return neck\n",
    "\n",
    "# heads\n",
    "gender = build_tower(neck, num_classes=2, activation=\"softmax\", name=\"gender_output\")\n",
    "image_quality = build_tower(neck, num_classes=3, activation=\"softmax\", name=\"image_quality_output\")\n",
    "age = build_tower(neck, num_classes=len(unique_ages), activation=\"softmax\", name=\"age_output\") \n",
    "weight = build_tower(neck, num_classes=len(unique_weight), activation=\"softmax\", name=\"weight_output\")\n",
    "bag = build_tower(neck, num_classes=len(unique_bags), activation=\"softmax\", name=\"bag_output\") \n",
    "footwear = build_tower(neck, num_classes=len(unique_footwears), activation=\"softmax\", name=\"footwear_output\") \n",
    "pose = build_tower(neck, num_classes=len(unique_poses), activation=\"softmax\", name=\"pose_output\")\n",
    "emotion =  build_tower(neck, num_classes=len(unique_emotions), activation=\"softmax\", name=\"emotion_output\") \n",
    "\n",
    "# freeze backbone\n",
    "for layer in backbone.layers:\n",
    "\tlayer.trainable = True\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    inputs=backbone.input, \n",
    "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200, 200, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 206, 206, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/conv (Conv2D)             (None, 100, 100, 64) 9408        zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1/bn (BatchNormalization)   (None, 100, 100, 64) 256         conv1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1/relu (Activation)         (None, 100, 100, 64) 0           conv1/bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 102, 102, 64) 0           conv1/relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 50, 50, 64)   0           zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 50, 50, 64)   256         pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_relu (Activation (None, 50, 50, 64)   0           conv2_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 50, 50, 128)  8192        conv2_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 50, 50, 128)  512         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 50, 50, 128)  0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 50, 50, 32)   36864       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_concat (Concatenat (None, 50, 50, 96)   0           pool1[0][0]                      \n",
      "                                                                 conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_bn (BatchNormali (None, 50, 50, 96)   384         conv2_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_relu (Activation (None, 50, 50, 96)   0           conv2_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 50, 50, 128)  12288       conv2_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 50, 50, 128)  512         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 50, 50, 128)  0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 50, 50, 32)   36864       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_concat (Concatenat (None, 50, 50, 128)  0           conv2_block1_concat[0][0]        \n",
      "                                                                 conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_bn (BatchNormali (None, 50, 50, 128)  512         conv2_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_relu (Activation (None, 50, 50, 128)  0           conv2_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 50, 50, 128)  16384       conv2_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 50, 50, 128)  512         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 50, 50, 128)  0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 50, 50, 32)   36864       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_concat (Concatenat (None, 50, 50, 160)  0           conv2_block2_concat[0][0]        \n",
      "                                                                 conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_bn (BatchNormali (None, 50, 50, 160)  640         conv2_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_relu (Activation (None, 50, 50, 160)  0           conv2_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_conv (Conv2D)    (None, 50, 50, 128)  20480       conv2_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_bn (BatchNormali (None, 50, 50, 128)  512         conv2_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_relu (Activation (None, 50, 50, 128)  0           conv2_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_2_conv (Conv2D)    (None, 50, 50, 32)   36864       conv2_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_concat (Concatenat (None, 50, 50, 192)  0           conv2_block3_concat[0][0]        \n",
      "                                                                 conv2_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_bn (BatchNormali (None, 50, 50, 192)  768         conv2_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_relu (Activation (None, 50, 50, 192)  0           conv2_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_conv (Conv2D)    (None, 50, 50, 128)  24576       conv2_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_bn (BatchNormali (None, 50, 50, 128)  512         conv2_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_relu (Activation (None, 50, 50, 128)  0           conv2_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_2_conv (Conv2D)    (None, 50, 50, 32)   36864       conv2_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_concat (Concatenat (None, 50, 50, 224)  0           conv2_block4_concat[0][0]        \n",
      "                                                                 conv2_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_bn (BatchNormali (None, 50, 50, 224)  896         conv2_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_relu (Activation (None, 50, 50, 224)  0           conv2_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_conv (Conv2D)    (None, 50, 50, 128)  28672       conv2_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_bn (BatchNormali (None, 50, 50, 128)  512         conv2_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_relu (Activation (None, 50, 50, 128)  0           conv2_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_2_conv (Conv2D)    (None, 50, 50, 32)   36864       conv2_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_concat (Concatenat (None, 50, 50, 256)  0           conv2_block5_concat[0][0]        \n",
      "                                                                 conv2_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_bn (BatchNormalization)   (None, 50, 50, 256)  1024        conv2_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_relu (Activation)         (None, 50, 50, 256)  0           pool2_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool2_conv (Conv2D)             (None, 50, 50, 128)  32768       pool2_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool2_pool (AveragePooling2D)   (None, 25, 25, 128)  0           pool2_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 25, 25, 128)  512         pool2_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_relu (Activation (None, 25, 25, 128)  0           conv3_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 25, 25, 128)  16384       conv3_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 25, 25, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 25, 25, 32)   36864       conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_concat (Concatenat (None, 25, 25, 160)  0           pool2_pool[0][0]                 \n",
      "                                                                 conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_bn (BatchNormali (None, 25, 25, 160)  640         conv3_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_relu (Activation (None, 25, 25, 160)  0           conv3_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 25, 25, 128)  20480       conv3_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 25, 25, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 25, 25, 32)   36864       conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_concat (Concatenat (None, 25, 25, 192)  0           conv3_block1_concat[0][0]        \n",
      "                                                                 conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_bn (BatchNormali (None, 25, 25, 192)  768         conv3_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_relu (Activation (None, 25, 25, 192)  0           conv3_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 25, 25, 128)  24576       conv3_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 25, 25, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 25, 25, 32)   36864       conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_concat (Concatenat (None, 25, 25, 224)  0           conv3_block2_concat[0][0]        \n",
      "                                                                 conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_bn (BatchNormali (None, 25, 25, 224)  896         conv3_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_relu (Activation (None, 25, 25, 224)  0           conv3_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 25, 25, 128)  28672       conv3_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 25, 25, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 25, 25, 32)   36864       conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_concat (Concatenat (None, 25, 25, 256)  0           conv3_block3_concat[0][0]        \n",
      "                                                                 conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_bn (BatchNormali (None, 25, 25, 256)  1024        conv3_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_relu (Activation (None, 25, 25, 256)  0           conv3_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)    (None, 25, 25, 128)  32768       conv3_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation (None, 25, 25, 128)  0           conv3_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)    (None, 25, 25, 32)   36864       conv3_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_concat (Concatenat (None, 25, 25, 288)  0           conv3_block4_concat[0][0]        \n",
      "                                                                 conv3_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_bn (BatchNormali (None, 25, 25, 288)  1152        conv3_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_relu (Activation (None, 25, 25, 288)  0           conv3_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)    (None, 25, 25, 128)  36864       conv3_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation (None, 25, 25, 128)  0           conv3_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)    (None, 25, 25, 32)   36864       conv3_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_concat (Concatenat (None, 25, 25, 320)  0           conv3_block5_concat[0][0]        \n",
      "                                                                 conv3_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_bn (BatchNormali (None, 25, 25, 320)  1280        conv3_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_relu (Activation (None, 25, 25, 320)  0           conv3_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)    (None, 25, 25, 128)  40960       conv3_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation (None, 25, 25, 128)  0           conv3_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)    (None, 25, 25, 32)   36864       conv3_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_concat (Concatenat (None, 25, 25, 352)  0           conv3_block6_concat[0][0]        \n",
      "                                                                 conv3_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_bn (BatchNormali (None, 25, 25, 352)  1408        conv3_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_relu (Activation (None, 25, 25, 352)  0           conv3_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)    (None, 25, 25, 128)  45056       conv3_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation (None, 25, 25, 128)  0           conv3_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)    (None, 25, 25, 32)   36864       conv3_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_concat (Concatenat (None, 25, 25, 384)  0           conv3_block7_concat[0][0]        \n",
      "                                                                 conv3_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_bn (BatchNormali (None, 25, 25, 384)  1536        conv3_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_relu (Activation (None, 25, 25, 384)  0           conv3_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_conv (Conv2D)    (None, 25, 25, 128)  49152       conv3_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_bn (BatchNormali (None, 25, 25, 128)  512         conv3_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_relu (Activation (None, 25, 25, 128)  0           conv3_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_2_conv (Conv2D)    (None, 25, 25, 32)   36864       conv3_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_concat (Concatenat (None, 25, 25, 416)  0           conv3_block8_concat[0][0]        \n",
      "                                                                 conv3_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_bn (BatchNormal (None, 25, 25, 416)  1664        conv3_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_relu (Activatio (None, 25, 25, 416)  0           conv3_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_conv (Conv2D)   (None, 25, 25, 128)  53248       conv3_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_bn (BatchNormal (None, 25, 25, 128)  512         conv3_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_relu (Activatio (None, 25, 25, 128)  0           conv3_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_2_conv (Conv2D)   (None, 25, 25, 32)   36864       conv3_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_concat (Concatena (None, 25, 25, 448)  0           conv3_block9_concat[0][0]        \n",
      "                                                                 conv3_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_bn (BatchNormal (None, 25, 25, 448)  1792        conv3_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_relu (Activatio (None, 25, 25, 448)  0           conv3_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_conv (Conv2D)   (None, 25, 25, 128)  57344       conv3_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_bn (BatchNormal (None, 25, 25, 128)  512         conv3_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_relu (Activatio (None, 25, 25, 128)  0           conv3_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_2_conv (Conv2D)   (None, 25, 25, 32)   36864       conv3_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_concat (Concatena (None, 25, 25, 480)  0           conv3_block10_concat[0][0]       \n",
      "                                                                 conv3_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_bn (BatchNormal (None, 25, 25, 480)  1920        conv3_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_relu (Activatio (None, 25, 25, 480)  0           conv3_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_conv (Conv2D)   (None, 25, 25, 128)  61440       conv3_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_bn (BatchNormal (None, 25, 25, 128)  512         conv3_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_relu (Activatio (None, 25, 25, 128)  0           conv3_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_2_conv (Conv2D)   (None, 25, 25, 32)   36864       conv3_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_concat (Concatena (None, 25, 25, 512)  0           conv3_block11_concat[0][0]       \n",
      "                                                                 conv3_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_bn (BatchNormalization)   (None, 25, 25, 512)  2048        conv3_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_relu (Activation)         (None, 25, 25, 512)  0           pool3_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool3_conv (Conv2D)             (None, 25, 25, 256)  131072      pool3_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool3_pool (AveragePooling2D)   (None, 12, 12, 256)  0           pool3_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 12, 12, 256)  1024        pool3_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_relu (Activation (None, 12, 12, 256)  0           conv4_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 12, 12, 128)  32768       conv4_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 12, 12, 128)  512         conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 12, 12, 128)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 12, 12, 32)   36864       conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_concat (Concatenat (None, 12, 12, 288)  0           pool3_pool[0][0]                 \n",
      "                                                                 conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_bn (BatchNormali (None, 12, 12, 288)  1152        conv4_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_relu (Activation (None, 12, 12, 288)  0           conv4_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 12, 12, 128)  36864       conv4_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 12, 12, 128)  512         conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 12, 12, 128)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 12, 12, 32)   36864       conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_concat (Concatenat (None, 12, 12, 320)  0           conv4_block1_concat[0][0]        \n",
      "                                                                 conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_bn (BatchNormali (None, 12, 12, 320)  1280        conv4_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_relu (Activation (None, 12, 12, 320)  0           conv4_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 12, 12, 128)  40960       conv4_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 12, 12, 128)  512         conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 12, 12, 128)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 12, 12, 32)   36864       conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_concat (Concatenat (None, 12, 12, 352)  0           conv4_block2_concat[0][0]        \n",
      "                                                                 conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_bn (BatchNormali (None, 12, 12, 352)  1408        conv4_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_relu (Activation (None, 12, 12, 352)  0           conv4_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 12, 12, 128)  45056       conv4_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 12, 12, 128)  512         conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 12, 12, 128)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 12, 12, 32)   36864       conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_concat (Concatenat (None, 12, 12, 384)  0           conv4_block3_concat[0][0]        \n",
      "                                                                 conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_bn (BatchNormali (None, 12, 12, 384)  1536        conv4_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_relu (Activation (None, 12, 12, 384)  0           conv4_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 12, 12, 128)  49152       conv4_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 12, 12, 128)  512         conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 12, 12, 128)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 12, 12, 32)   36864       conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_concat (Concatenat (None, 12, 12, 416)  0           conv4_block4_concat[0][0]        \n",
      "                                                                 conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_bn (BatchNormali (None, 12, 12, 416)  1664        conv4_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_relu (Activation (None, 12, 12, 416)  0           conv4_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 12, 12, 128)  53248       conv4_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 12, 12, 128)  512         conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 12, 12, 128)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 12, 12, 32)   36864       conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_concat (Concatenat (None, 12, 12, 448)  0           conv4_block5_concat[0][0]        \n",
      "                                                                 conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_bn (BatchNormali (None, 12, 12, 448)  1792        conv4_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_relu (Activation (None, 12, 12, 448)  0           conv4_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 12, 12, 128)  57344       conv4_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 12, 12, 128)  512         conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 12, 12, 128)  0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 12, 12, 32)   36864       conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_concat (Concatenat (None, 12, 12, 480)  0           conv4_block6_concat[0][0]        \n",
      "                                                                 conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_bn (BatchNormali (None, 12, 12, 480)  1920        conv4_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_relu (Activation (None, 12, 12, 480)  0           conv4_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 12, 12, 128)  61440       conv4_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 12, 12, 128)  512         conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 12, 12, 128)  0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 12, 12, 32)   36864       conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_concat (Concatenat (None, 12, 12, 512)  0           conv4_block7_concat[0][0]        \n",
      "                                                                 conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_bn (BatchNormali (None, 12, 12, 512)  2048        conv4_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_relu (Activation (None, 12, 12, 512)  0           conv4_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 12, 12, 128)  65536       conv4_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 12, 12, 128)  512         conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 12, 12, 128)  0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 12, 12, 32)   36864       conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_concat (Concatenat (None, 12, 12, 544)  0           conv4_block8_concat[0][0]        \n",
      "                                                                 conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_bn (BatchNormal (None, 12, 12, 544)  2176        conv4_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_relu (Activatio (None, 12, 12, 544)  0           conv4_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 12, 12, 128)  69632       conv4_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_concat (Concatena (None, 12, 12, 576)  0           conv4_block9_concat[0][0]        \n",
      "                                                                 conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_bn (BatchNormal (None, 12, 12, 576)  2304        conv4_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_relu (Activatio (None, 12, 12, 576)  0           conv4_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 12, 12, 128)  73728       conv4_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_concat (Concatena (None, 12, 12, 608)  0           conv4_block10_concat[0][0]       \n",
      "                                                                 conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_bn (BatchNormal (None, 12, 12, 608)  2432        conv4_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_relu (Activatio (None, 12, 12, 608)  0           conv4_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 12, 12, 128)  77824       conv4_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_concat (Concatena (None, 12, 12, 640)  0           conv4_block11_concat[0][0]       \n",
      "                                                                 conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_bn (BatchNormal (None, 12, 12, 640)  2560        conv4_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_relu (Activatio (None, 12, 12, 640)  0           conv4_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 12, 12, 128)  81920       conv4_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_concat (Concatena (None, 12, 12, 672)  0           conv4_block12_concat[0][0]       \n",
      "                                                                 conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_bn (BatchNormal (None, 12, 12, 672)  2688        conv4_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_relu (Activatio (None, 12, 12, 672)  0           conv4_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 12, 12, 128)  86016       conv4_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_concat (Concatena (None, 12, 12, 704)  0           conv4_block13_concat[0][0]       \n",
      "                                                                 conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_bn (BatchNormal (None, 12, 12, 704)  2816        conv4_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_relu (Activatio (None, 12, 12, 704)  0           conv4_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 12, 12, 128)  90112       conv4_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_concat (Concatena (None, 12, 12, 736)  0           conv4_block14_concat[0][0]       \n",
      "                                                                 conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_bn (BatchNormal (None, 12, 12, 736)  2944        conv4_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_relu (Activatio (None, 12, 12, 736)  0           conv4_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 12, 12, 128)  94208       conv4_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_concat (Concatena (None, 12, 12, 768)  0           conv4_block15_concat[0][0]       \n",
      "                                                                 conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_bn (BatchNormal (None, 12, 12, 768)  3072        conv4_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_relu (Activatio (None, 12, 12, 768)  0           conv4_block17_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, 12, 12, 128)  98304       conv4_block17_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_concat (Concatena (None, 12, 12, 800)  0           conv4_block16_concat[0][0]       \n",
      "                                                                 conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_bn (BatchNormal (None, 12, 12, 800)  3200        conv4_block17_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_relu (Activatio (None, 12, 12, 800)  0           conv4_block18_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, 12, 12, 128)  102400      conv4_block18_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_concat (Concatena (None, 12, 12, 832)  0           conv4_block17_concat[0][0]       \n",
      "                                                                 conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_bn (BatchNormal (None, 12, 12, 832)  3328        conv4_block18_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_relu (Activatio (None, 12, 12, 832)  0           conv4_block19_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, 12, 12, 128)  106496      conv4_block19_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_concat (Concatena (None, 12, 12, 864)  0           conv4_block18_concat[0][0]       \n",
      "                                                                 conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_bn (BatchNormal (None, 12, 12, 864)  3456        conv4_block19_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_relu (Activatio (None, 12, 12, 864)  0           conv4_block20_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, 12, 12, 128)  110592      conv4_block20_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_concat (Concatena (None, 12, 12, 896)  0           conv4_block19_concat[0][0]       \n",
      "                                                                 conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_bn (BatchNormal (None, 12, 12, 896)  3584        conv4_block20_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_relu (Activatio (None, 12, 12, 896)  0           conv4_block21_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, 12, 12, 128)  114688      conv4_block21_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_concat (Concatena (None, 12, 12, 928)  0           conv4_block20_concat[0][0]       \n",
      "                                                                 conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_bn (BatchNormal (None, 12, 12, 928)  3712        conv4_block21_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_relu (Activatio (None, 12, 12, 928)  0           conv4_block22_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, 12, 12, 128)  118784      conv4_block22_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_concat (Concatena (None, 12, 12, 960)  0           conv4_block21_concat[0][0]       \n",
      "                                                                 conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_bn (BatchNormal (None, 12, 12, 960)  3840        conv4_block22_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_relu (Activatio (None, 12, 12, 960)  0           conv4_block23_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, 12, 12, 128)  122880      conv4_block23_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_concat (Concatena (None, 12, 12, 992)  0           conv4_block22_concat[0][0]       \n",
      "                                                                 conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_bn (BatchNormal (None, 12, 12, 992)  3968        conv4_block23_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_relu (Activatio (None, 12, 12, 992)  0           conv4_block24_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)   (None, 12, 12, 128)  126976      conv4_block24_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormal (None, 12, 12, 128)  512         conv4_block24_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activatio (None, 12, 12, 128)  0           conv4_block24_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)   (None, 12, 12, 32)   36864       conv4_block24_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_concat (Concatena (None, 12, 12, 1024) 0           conv4_block23_concat[0][0]       \n",
      "                                                                 conv4_block24_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_bn (BatchNormalization)   (None, 12, 12, 1024) 4096        conv4_block24_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_relu (Activation)         (None, 12, 12, 1024) 0           pool4_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool4_conv (Conv2D)             (None, 12, 12, 512)  524288      pool4_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool4_pool (AveragePooling2D)   (None, 6, 6, 512)    0           pool4_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 6, 6, 512)    2048        pool4_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_relu (Activation (None, 6, 6, 512)    0           conv5_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 6, 6, 128)    65536       conv5_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 6, 6, 128)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_concat (Concatenat (None, 6, 6, 544)    0           pool4_pool[0][0]                 \n",
      "                                                                 conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_bn (BatchNormali (None, 6, 6, 544)    2176        conv5_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_relu (Activation (None, 6, 6, 544)    0           conv5_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 6, 6, 128)    69632       conv5_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 6, 6, 128)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_concat (Concatenat (None, 6, 6, 576)    0           conv5_block1_concat[0][0]        \n",
      "                                                                 conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_bn (BatchNormali (None, 6, 6, 576)    2304        conv5_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_relu (Activation (None, 6, 6, 576)    0           conv5_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 6, 6, 128)    73728       conv5_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 6, 6, 128)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_concat (Concatenat (None, 6, 6, 608)    0           conv5_block2_concat[0][0]        \n",
      "                                                                 conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_bn (BatchNormali (None, 6, 6, 608)    2432        conv5_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_relu (Activation (None, 6, 6, 608)    0           conv5_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_conv (Conv2D)    (None, 6, 6, 128)    77824       conv5_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_relu (Activation (None, 6, 6, 128)    0           conv5_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_concat (Concatenat (None, 6, 6, 640)    0           conv5_block3_concat[0][0]        \n",
      "                                                                 conv5_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_bn (BatchNormali (None, 6, 6, 640)    2560        conv5_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_relu (Activation (None, 6, 6, 640)    0           conv5_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_conv (Conv2D)    (None, 6, 6, 128)    81920       conv5_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_relu (Activation (None, 6, 6, 128)    0           conv5_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_concat (Concatenat (None, 6, 6, 672)    0           conv5_block4_concat[0][0]        \n",
      "                                                                 conv5_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_bn (BatchNormali (None, 6, 6, 672)    2688        conv5_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_relu (Activation (None, 6, 6, 672)    0           conv5_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_conv (Conv2D)    (None, 6, 6, 128)    86016       conv5_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_relu (Activation (None, 6, 6, 128)    0           conv5_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_concat (Concatenat (None, 6, 6, 704)    0           conv5_block5_concat[0][0]        \n",
      "                                                                 conv5_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_bn (BatchNormali (None, 6, 6, 704)    2816        conv5_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_relu (Activation (None, 6, 6, 704)    0           conv5_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_conv (Conv2D)    (None, 6, 6, 128)    90112       conv5_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_relu (Activation (None, 6, 6, 128)    0           conv5_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_concat (Concatenat (None, 6, 6, 736)    0           conv5_block6_concat[0][0]        \n",
      "                                                                 conv5_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_bn (BatchNormali (None, 6, 6, 736)    2944        conv5_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_relu (Activation (None, 6, 6, 736)    0           conv5_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_conv (Conv2D)    (None, 6, 6, 128)    94208       conv5_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_relu (Activation (None, 6, 6, 128)    0           conv5_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_concat (Concatenat (None, 6, 6, 768)    0           conv5_block7_concat[0][0]        \n",
      "                                                                 conv5_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_bn (BatchNormali (None, 6, 6, 768)    3072        conv5_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_relu (Activation (None, 6, 6, 768)    0           conv5_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_conv (Conv2D)    (None, 6, 6, 128)    98304       conv5_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_bn (BatchNormali (None, 6, 6, 128)    512         conv5_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_relu (Activation (None, 6, 6, 128)    0           conv5_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_2_conv (Conv2D)    (None, 6, 6, 32)     36864       conv5_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_concat (Concatenat (None, 6, 6, 800)    0           conv5_block8_concat[0][0]        \n",
      "                                                                 conv5_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_bn (BatchNormal (None, 6, 6, 800)    3200        conv5_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_relu (Activatio (None, 6, 6, 800)    0           conv5_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_conv (Conv2D)   (None, 6, 6, 128)    102400      conv5_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_concat (Concatena (None, 6, 6, 832)    0           conv5_block9_concat[0][0]        \n",
      "                                                                 conv5_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_bn (BatchNormal (None, 6, 6, 832)    3328        conv5_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_relu (Activatio (None, 6, 6, 832)    0           conv5_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_conv (Conv2D)   (None, 6, 6, 128)    106496      conv5_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_concat (Concatena (None, 6, 6, 864)    0           conv5_block10_concat[0][0]       \n",
      "                                                                 conv5_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_bn (BatchNormal (None, 6, 6, 864)    3456        conv5_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_relu (Activatio (None, 6, 6, 864)    0           conv5_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_conv (Conv2D)   (None, 6, 6, 128)    110592      conv5_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_concat (Concatena (None, 6, 6, 896)    0           conv5_block11_concat[0][0]       \n",
      "                                                                 conv5_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_bn (BatchNormal (None, 6, 6, 896)    3584        conv5_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_relu (Activatio (None, 6, 6, 896)    0           conv5_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv5_block13_1_conv (Conv2D)   (None, 6, 6, 128)    114688      conv5_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_concat (Concatena (None, 6, 6, 928)    0           conv5_block12_concat[0][0]       \n",
      "                                                                 conv5_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_bn (BatchNormal (None, 6, 6, 928)    3712        conv5_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_relu (Activatio (None, 6, 6, 928)    0           conv5_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_conv (Conv2D)   (None, 6, 6, 128)    118784      conv5_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_concat (Concatena (None, 6, 6, 960)    0           conv5_block13_concat[0][0]       \n",
      "                                                                 conv5_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_bn (BatchNormal (None, 6, 6, 960)    3840        conv5_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_relu (Activatio (None, 6, 6, 960)    0           conv5_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_conv (Conv2D)   (None, 6, 6, 128)    122880      conv5_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_concat (Concatena (None, 6, 6, 992)    0           conv5_block14_concat[0][0]       \n",
      "                                                                 conv5_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_bn (BatchNormal (None, 6, 6, 992)    3968        conv5_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_relu (Activatio (None, 6, 6, 992)    0           conv5_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_conv (Conv2D)   (None, 6, 6, 128)    126976      conv5_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_bn (BatchNormal (None, 6, 6, 128)    512         conv5_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_relu (Activatio (None, 6, 6, 128)    0           conv5_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_2_conv (Conv2D)   (None, 6, 6, 32)     36864       conv5_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_concat (Concatena (None, 6, 6, 1024)   0           conv5_block15_concat[0][0]       \n",
      "                                                                 conv5_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, 6, 6, 1024)   4096        conv5_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "relu (Activation)               (None, 6, 6, 1024)   0           bn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 6, 6, 2)      2048        relu[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 6, 6, 3)      3072        relu[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 6, 6, 5)      5120        relu[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 6, 6, 4)      4096        relu[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 6, 6, 3)      3072        relu[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 6, 6, 3)      3072        relu[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 6, 6, 3)      3072        relu[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 6, 6, 4)      4096        relu[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2)            0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 3)            0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 5)            0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 4)            0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 3)            0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_6 (Glo (None, 3)            0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_7 (Glo (None, 3)            0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_8 (Glo (None, 4)            0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gender_output (Activation)      (None, 2)            0           global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "image_quality_output (Activatio (None, 3)            0           global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "age_output (Activation)         (None, 5)            0           global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "weight_output (Activation)      (None, 4)            0           global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "bag_output (Activation)         (None, 3)            0           global_average_pooling2d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "footwear_output (Activation)    (None, 3)            0           global_average_pooling2d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "pose_output (Activation)        (None, 3)            0           global_average_pooling2d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "emotion_output (Activation)     (None, 4)            0           global_average_pooling2d_8[0][0] \n",
      "==================================================================================================\n",
      "Total params: 7,065,152\n",
      "Trainable params: 6,981,504\n",
      "Non-trainable params: 83,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RfPG9C2eA1zn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet121\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "losses = {\n",
    "    \"gender_output\": \"categorical_crossentropy\",\n",
    "    \"image_quality_output\": \"categorical_crossentropy\",\n",
    "    \"age_output\": \"categorical_crossentropy\",\n",
    "    \"weight_output\": \"categorical_crossentropy\",\n",
    "    \"bag_output\": \"categorical_crossentropy\", \n",
    "    \"footwear_output\": \"categorical_crossentropy\", \n",
    "    \"pose_output\": \"categorical_crossentropy\", \n",
    "    \"emotion_output\": \"categorical_crossentropy\"\n",
    "}\n",
    "\n",
    "\n",
    "loss_weights =  {\n",
    "                    \"gender_output\": 1.0, \n",
    "                    \"image_quality_output\": 1.0, \n",
    "                    \"age_output\": 1.0, \n",
    "                    \"weight_output\": 1.0, \n",
    "                    \"bag_output\": 1.0, \n",
    "                    \"footwear_output\": 1.0, \n",
    "                    \"pose_output\": 1.0, \n",
    "                    \"emotion_output\": 1.0\n",
    "                }\n",
    "\n",
    "model.compile(loss=losses,\n",
    "              optimizer = SGD(momentum=0.9, nesterov=True),\n",
    "              #optimizer=Adam(),              \n",
    "              loss_weights=loss_weights,\n",
    "              metrics=['accuracy'])\n",
    "print(model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Following few cells, are executed separately after model compilation to find the 'max_lr' parameter (after zooming in to the loss plot with learning rate corresponding to the start of minimum value for loss plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  3/636 [..............................] - ETA: 137:21:08 - loss: 4.4784 - gender_output_loss: 0.3925 - image_quality_output_loss: 0.6151 - age_output_loss: 0.6811 - weight_output_loss: 0.5443 - bag_output_loss: 0.5950 - footwear_output_loss: 0.5215 - pose_output_loss: 0.3598 - emotion_output_loss: 0.7691 - gender_output_accuracy: 0.5417 - image_quality_output_accuracy: 0.2500 - age_output_accuracy: 0.1667 - weight_output_accuracy: 0.3542 - bag_output_accuracy: 0.2917 - footwear_output_accuracy: 0.2708 - pose_output_accuracy: 0.4792 - emotion_output_accuracy: 0.1667"
     ]
    }
   ],
   "source": [
    "lr_callback = LRFinder(num_samples=x_train.shape[0], batch_size=batch_size,\n",
    "                       minimum_lr=0.00000002, maximum_lr=25.0,verbose=False,\n",
    "                       lr_scale='exp', save_dir='./LR_LOGS/')\n",
    " \n",
    "# Need to have the epochs = 1 here.\n",
    "train_history = model.fit_generator(train_iterator,\n",
    "                                    steps_per_epoch=x_train.shape[0]//batch_size, \n",
    "                                    epochs=1, \n",
    "                                    validation_data = test_iterator,\n",
    "                                    validation_steps=x_test.shape[0]//batch_size,\n",
    "                                    callbacks=[lr_callback],\n",
    "                                    class_weight=class_weights,shuffle=True,\n",
    "                                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1227 13:20:45.253221 22580 deprecation_wrapper.py:119] From C:\\Users\\ojhaj\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " - 305s - loss: 2.1456 - gender_output_loss: 0.2750 - image_quality_output_loss: 0.3113 - age_output_loss: 0.3219 - weight_output_loss: 0.2327 - bag_output_loss: 0.3064 - footwear_output_loss: 0.3063 - pose_output_loss: 0.2308 - emotion_output_loss: 0.1612 - gender_output_accuracy: 0.7046 - image_quality_output_accuracy: 0.5455 - age_output_accuracy: 0.3971 - weight_output_accuracy: 0.6286 - bag_output_accuracy: 0.5750 - footwear_output_accuracy: 0.5603 - pose_output_accuracy: 0.6190 - emotion_output_accuracy: 0.7096 - val_loss: 7.1949 - val_gender_output_loss: 0.4308 - val_image_quality_output_loss: 1.0303 - val_age_output_loss: 1.8868 - val_weight_output_loss: 1.1549 - val_bag_output_loss: 0.9673 - val_footwear_output_loss: 0.9162 - val_pose_output_loss: 0.7493 - val_emotion_output_loss: 1.1388 - val_gender_output_accuracy: 0.8119 - val_image_quality_output_accuracy: 0.5528 - val_age_output_accuracy: 0.3880 - val_weight_output_accuracy: 0.6241 - val_bag_output_accuracy: 0.5999 - val_footwear_output_accuracy: 0.6468 - val_pose_output_accuracy: 0.6984 - val_emotion_output_accuracy: 0.7129\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 7.19495, saving model to D:\\PG-ML\\eip\\week5\\saved_models\\PersonAttrib_DenseNet121_ep-001va-loss-7.194950gn-0.811910iq-0.552771ag-0.387972wt-0.624116bg-0.599941fw-0.646816ps-0.698408em-0.712854.h5\n",
      " - lr: 0.00089 \n",
      "Epoch 2/5\n",
      " - 293s - loss: 1.8090 - gender_output_loss: 0.1938 - image_quality_output_loss: 0.2790 - age_output_loss: 0.2942 - weight_output_loss: 0.2109 - bag_output_loss: 0.2646 - footwear_output_loss: 0.2562 - pose_output_loss: 0.1702 - emotion_output_loss: 0.1401 - gender_output_accuracy: 0.8235 - image_quality_output_accuracy: 0.5625 - age_output_accuracy: 0.4148 - weight_output_accuracy: 0.6451 - bag_output_accuracy: 0.6265 - footwear_output_accuracy: 0.6506 - pose_output_accuracy: 0.7142 - emotion_output_accuracy: 0.7169 - val_loss: 7.8308 - val_gender_output_loss: 0.4906 - val_image_quality_output_loss: 1.0972 - val_age_output_loss: 1.6286 - val_weight_output_loss: 1.4250 - val_bag_output_loss: 0.9512 - val_footwear_output_loss: 0.8283 - val_pose_output_loss: 0.6236 - val_emotion_output_loss: 1.2519 - val_gender_output_accuracy: 0.8022 - val_image_quality_output_accuracy: 0.5681 - val_age_output_accuracy: 0.4068 - val_weight_output_accuracy: 0.6203 - val_bag_output_accuracy: 0.6279 - val_footwear_output_accuracy: 0.6521 - val_pose_output_accuracy: 0.7577 - val_emotion_output_accuracy: 0.7008\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 7.19495\n",
      " - lr: 0.00160 \n",
      "Epoch 3/5\n",
      " - 297s - loss: 1.7064 - gender_output_loss: 0.1727 - image_quality_output_loss: 0.2663 - age_output_loss: 0.2833 - weight_output_loss: 0.2066 - bag_output_loss: 0.2526 - footwear_output_loss: 0.2329 - pose_output_loss: 0.1540 - emotion_output_loss: 0.1381 - gender_output_accuracy: 0.8460 - image_quality_output_accuracy: 0.5877 - age_output_accuracy: 0.4254 - weight_output_accuracy: 0.6384 - bag_output_accuracy: 0.6484 - footwear_output_accuracy: 0.6824 - pose_output_accuracy: 0.7506 - emotion_output_accuracy: 0.7184 - val_loss: 8.0456 - val_gender_output_loss: 0.3388 - val_image_quality_output_loss: 1.0611 - val_age_output_loss: 1.4998 - val_weight_output_loss: 1.1525 - val_bag_output_loss: 0.9271 - val_footwear_output_loss: 0.7988 - val_pose_output_loss: 0.4795 - val_emotion_output_loss: 1.2572 - val_gender_output_accuracy: 0.8753 - val_image_quality_output_accuracy: 0.5817 - val_age_output_accuracy: 0.3597 - val_weight_output_accuracy: 0.6380 - val_bag_output_accuracy: 0.6442 - val_footwear_output_accuracy: 0.6731 - val_pose_output_accuracy: 0.8190 - val_emotion_output_accuracy: 0.7161\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 7.19495\n",
      " - lr: 0.00125 \n",
      "Epoch 4/5\n",
      " - 294s - loss: 1.5988 - gender_output_loss: 0.1487 - image_quality_output_loss: 0.2512 - age_output_loss: 0.2722 - weight_output_loss: 0.1980 - bag_output_loss: 0.2325 - footwear_output_loss: 0.2178 - pose_output_loss: 0.1407 - emotion_output_loss: 0.1376 - gender_output_accuracy: 0.8703 - image_quality_output_accuracy: 0.6053 - age_output_accuracy: 0.4468 - weight_output_accuracy: 0.6418 - bag_output_accuracy: 0.6755 - footwear_output_accuracy: 0.6963 - pose_output_accuracy: 0.7758 - emotion_output_accuracy: 0.7101 - val_loss: 7.6940 - val_gender_output_loss: 0.3251 - val_image_quality_output_loss: 0.9835 - val_age_output_loss: 1.4676 - val_weight_output_loss: 1.1282 - val_bag_output_loss: 0.9782 - val_footwear_output_loss: 0.7998 - val_pose_output_loss: 0.5940 - val_emotion_output_loss: 1.0753 - val_gender_output_accuracy: 0.8591 - val_image_quality_output_accuracy: 0.5787 - val_age_output_accuracy: 0.3939 - val_weight_output_accuracy: 0.6229 - val_bag_output_accuracy: 0.6159 - val_footwear_output_accuracy: 0.6607 - val_pose_output_accuracy: 0.7933 - val_emotion_output_accuracy: 0.7067\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 7.19495\n",
      " - lr: 0.00053 \n",
      "Epoch 5/5\n",
      " - 295s - loss: 1.5216 - gender_output_loss: 0.1323 - image_quality_output_loss: 0.2427 - age_output_loss: 0.2613 - weight_output_loss: 0.1892 - bag_output_loss: 0.2187 - footwear_output_loss: 0.2076 - pose_output_loss: 0.1330 - emotion_output_loss: 0.1368 - gender_output_accuracy: 0.8865 - image_quality_output_accuracy: 0.6128 - age_output_accuracy: 0.4606 - weight_output_accuracy: 0.6520 - bag_output_accuracy: 0.6973 - footwear_output_accuracy: 0.7048 - pose_output_accuracy: 0.7857 - emotion_output_accuracy: 0.7074 - val_loss: 6.6937 - val_gender_output_loss: 0.3004 - val_image_quality_output_loss: 0.9713 - val_age_output_loss: 1.4443 - val_weight_output_loss: 1.1251 - val_bag_output_loss: 0.9218 - val_footwear_output_loss: 0.7918 - val_pose_output_loss: 0.4694 - val_emotion_output_loss: 1.0714 - val_gender_output_accuracy: 0.8794 - val_image_quality_output_accuracy: 0.5787 - val_age_output_accuracy: 0.4113 - val_weight_output_accuracy: 0.6477 - val_bag_output_accuracy: 0.6527 - val_footwear_output_accuracy: 0.6769 - val_pose_output_accuracy: 0.8234 - val_emotion_output_accuracy: 0.7255\n",
      "\n",
      "Epoch 00005: val_loss improved from 7.19495 to 6.69365, saving model to D:\\PG-ML\\eip\\week5\\saved_models\\PersonAttrib_DenseNet121_ep-005va-loss-6.693653gn-0.879422iq-0.578715ag-0.411262wt-0.647700bg-0.652712fw-0.676887ps-0.823408em-0.725531.h5\n",
      " - lr: 0.00000 \n"
     ]
    }
   ],
   "source": [
    "# Prepare model model saving directory.\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'PersonAttrib_%s_ep-{epoch:03d}va-loss-{val_loss:03f}gn-{val_gender_output_accuracy:03f}iq-{val_image_quality_output_accuracy:03f}ag-{val_age_output_accuracy:03f}wt-{val_weight_output_accuracy:03f}bg-{val_bag_output_accuracy:03f}fw-{val_footwear_output_accuracy:03f}ps-{val_pose_output_accuracy:03f}em-{val_emotion_output_accuracy:03f}.h5' % model_type\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "#train_iterator = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "lr_manager = OneCycleLR(samples=x_train.shape[0], epochs=epochs, batch_size=batch_size,\n",
    "                    steps=x_train.shape[0]//batch_size, max_lr=0.001778279410038922,\n",
    "                    end_percentage=0.1, scale_percentage=None,\n",
    "                    maximum_momentum=None, minimum_momentum=None)\n",
    "callbacks = [checkpoint, lr_manager]\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "model_info = model.fit_generator(train_iterator,\n",
    "                                 validation_data = test_iterator,\n",
    "                                 steps_per_epoch=x_train.shape[0]//batch_size,\n",
    "                                 validation_steps=x_test.shape[0]//batch_size,\n",
    "                                 epochs=epochs, verbose=2,\n",
    "                                 class_weight=class_weights,shuffle=True,\n",
    "                                 callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PersonAttrubutes.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
